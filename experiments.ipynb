{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDRFejHvDBM8",
    "outputId": "089878cd-d587-4d1f-da98-c96049576aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
      "\u001b[K     |████████████████████████████████| 264 kB 14.4 MB/s \n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 73.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Collecting tqdm>=4.42\n",
      "  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 5.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.15-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
      "\u001b[?25hCollecting fsspec>=2021.05.0\n",
      "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 84.3 MB/s \n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 65.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 76.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 75.4 MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 60.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: tqdm, xxhash, tokenizers, sacremoses, pyyaml, huggingface-hub, fsspec, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed datasets-1.11.0 fsspec-2021.7.0 huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 tqdm-4.62.0 transformers-4.9.2 xxhash-2.0.2\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzJNbCeQDLZ4"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import savetxt, loadtxt\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "import errno\n",
    "import os\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2IBi2KJKLXz"
   },
   "source": [
    "# Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeLJnzKYDRHa"
   },
   "outputs": [],
   "source": [
    "model_name = 'xlnet-large-cased'  # 'bert-large-uncased', 'gpt2', 'xlm-roberta-large', 'xlnet-large-cased', ('roberta-large')\n",
    "\n",
    "num_prime_tokens = 2 \n",
    "occur_prime_trigrams = 4 # Each unique priming trigram occurs x times in prime input\n",
    "prime_pause_str = '.' # (Punctuation) token (string) that seperates trigrams in prime input\n",
    "\n",
    "priming_AAB = True\n",
    "priming_ABA = True\n",
    "priming_ABB = True\n",
    "\n",
    "num_probe_tokens = 4\n",
    "\"\"\"This version works only if all probing patterns are True\"\"\"\n",
    "probing_AAB = True\n",
    "probing_ABA = True\n",
    "probing_ABB = True\n",
    "probing_ABCa = True\n",
    "probing_ABCb = True\n",
    "\n",
    "\"\"\"Not implemented - needs to be set to True\"\"\"\n",
    "real_probes = True\n",
    "\n",
    "experiment_cycles = 16*16\n",
    "\n",
    "custom_primes = False # Number of trigrams = num_prime_tokens**2 * occur_prime_trigrams\n",
    "custom_probes = False # Number of trigrams = num_probe_tokens**2 \n",
    "probes_eql_primes = False\n",
    "upload_required = True # Set to False if PPMI index is already available\n",
    "\n",
    "num_probe = num_probe_tokens**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te6S2jyaKY_5"
   },
   "source": [
    "# Custom Primes or Probes: Upload PMI Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKL0rWoXKVfu"
   },
   "outputs": [],
   "source": [
    "if upload_required and (custom_primes or custom_probes):\n",
    "  prime_positions = np.random.permutation(np.arange(num_probe*2))\n",
    "  probe_positions = prime_positions[:num_probe]\n",
    "  prime_positions = prime_positions[num_probe:]\n",
    "  print('Please upload PMI CSV file for ', model_name)\n",
    "  custom_CSV = list(files.upload())[0]\n",
    "  pmi_index = pd.read_csv(custom_CSV, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVWYqoJkKeZ1"
   },
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSMAD2a93QXW"
   },
   "outputs": [],
   "source": [
    "metainfo = ''\n",
    "\n",
    "if probing_ABCa and probing_ABCb:\n",
    "  probing_ABCb = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if model_name.lower().find('gpt2') != -1:\n",
    "  from transformers import GPT2LMHeadModel\n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "  special_tokens = np.array([None, 50256, \n",
    "                           tokenizer.convert_tokens_to_ids(prime_pause_str), None])# CLS (None), MASK (<|endoftext|>), pause, SEP (None)\n",
    "elif model_name.lower().find('xlnet') != -1:\n",
    "  xlnet = True\n",
    "  from transformers import XLNetLMHeadModel\n",
    "  model = XLNetLMHeadModel.from_pretrained(model_name)\n",
    "  special_tokens = np.array([17, tokenizer.mask_token_id, \n",
    "                           tokenizer.convert_tokens_to_ids(prime_pause_str), tokenizer.cls_token_id])# 17 (Start), MASK, pause, CLS\n",
    "else: \n",
    "  model = BertForMaskedLM.from_pretrained(model_name)\n",
    "  special_tokens = np.array([tokenizer.cls_token_id, tokenizer.mask_token_id, \n",
    "                           tokenizer.convert_tokens_to_ids(prime_pause_str), tokenizer.sep_token_id])# CLS, MASK, pause, SEP\n",
    "\n",
    "pauses = np.repeat(special_tokens[2], num_prime_tokens**2*occur_prime_trigrams)\n",
    "\n",
    "def token_selector(num_tkns, ids_xcl=[]):\n",
    "  ids = random.sample(set(range(len(tokenizer.vocab.keys())))-set(ids_xcl), num_tkns)\n",
    "  return ids\n",
    "\n",
    "def priming_probs(prime_inpt, fltr, p_pstn=-1):\n",
    "  cls_v = np.tile(special_tokens[0], (num_probe, 1)).flatten()\n",
    "  sep_v = np.tile(special_tokens[-1], (num_probe, 1)).flatten()\n",
    "  #print(prime_inpt, sep_v, sep='\\n')\n",
    "  if model_name.lower().find('gpt2') == -1: # <|endoftext|> not required for gpt2\n",
    "    if model_name.lower().find('xlnet') != -1: \n",
    "      clsX_v = np.tile(4, (num_probe, 1)).flatten() # cls token is used differently in xlnet\n",
    "      prime_inpt = np.insert(np.roll(np.insert(np.roll(np.insert(prime_inpt,0,clsX_v,axis=1),-1),0,sep_v,axis=1),-1),0,cls_v,axis=1)\n",
    "      p_pstn=-3\n",
    "    else: \n",
    "      prime_inpt = np.roll(np.insert(np.insert(prime_inpt,0,cls_v,axis=1),0,sep_v,axis=1),-1)\n",
    "      p_pstn=-2\n",
    "  #print(prime_inpt)\n",
    "  #print(fltr)\n",
    "  prime_inpt = torch.tensor(prime_inpt)\n",
    "  prime_inpt = prime_inpt.to('cuda')\n",
    "  model.to('cuda')\n",
    "  logits = model(prime_inpt)[0]\n",
    "  logits.to('cuda')\n",
    "  softmax = F.softmax(logits, dim = -1)\n",
    "  softmax.to('cuda')\n",
    "  softmax = softmax[:,p_pstn].cpu().detach().numpy()\n",
    "  return softmax[:,tuple(fltr)]\n",
    "\n",
    "df_AAB_priming = pd.DataFrame([])\n",
    "df_ABA_priming = pd.DataFrame([])\n",
    "df_ABB_priming = pd.DataFrame([])\n",
    "\n",
    "for i_exp in range(experiment_cycles):\n",
    "  probabilities = np.full((4,6,num_probe),None).astype('float32')\n",
    "\n",
    "  prime_AAB = prime_ABA = prime_ABB = np.array([]).astype('int64')\n",
    "\n",
    "  if custom_primes: \n",
    "    if priming_AAB:   \n",
    "      prime_AAB = pmi_index[pmi_index['Pattern'].str.contains('AAB')][:num_probe*2].reset_index(drop=True).filter(\n",
    "          items=['Position 1', 'Position 2','Position 3']).to_numpy()[(prime_positions),:]\n",
    "      prime_AAB = np.roll(np.insert(prime_AAB,0,pauses,axis=1),-1) # Insert pauses\n",
    "\n",
    "    if priming_ABA:\n",
    "      prime_ABA = pmi_index[pmi_index['Pattern'].str.contains('ABA')][:num_probe*2].reset_index(drop=True).filter(\n",
    "          items=['Position 1', 'Position 2','Position 3']).to_numpy()[(prime_positions),:]\n",
    "      prime_ABA = np.roll(np.insert(prime_ABA,0,pauses,axis=1),-1) # Insert pauses\n",
    "    \n",
    "    if priming_ABB:\n",
    "      prime_ABB = pmi_index[pmi_index['Pattern'].str.contains('ABB')][:num_probe*2].reset_index(drop=True).filter(\n",
    "          items=['Position 1', 'Position 2','Position 3']).to_numpy()[(prime_positions),:]\n",
    "      prime_ABB = np.roll(np.insert(prime_ABB,0,pauses,axis=1),-1) # Insert pauses\n",
    "\n",
    "  else: # random primes\n",
    "      prime_order = np.repeat(np.arange(num_prime_tokens**2),occur_prime_trigrams)\n",
    "      np.random.shuffle(prime_order)\n",
    "\n",
    "      ids_prime_a = token_selector(num_prime_tokens,special_tokens)\n",
    "      ids_prime_b = token_selector(num_prime_tokens,np.concatenate((special_tokens, ids_prime_a), axis=0))\n",
    "\n",
    "      combs_AB = np.transpose((np.repeat(ids_prime_a, len(ids_prime_b)), np.tile(ids_prime_b, len(ids_prime_a))))\n",
    "\n",
    "      if priming_AAB: prime_AAB = np.insert(combs_AB[prime_order][:,(0,0,1)],3,pauses,axis=1)\n",
    "      if priming_ABA: prime_ABA = np.insert(combs_AB[prime_order][:,(0,1,0)],3,pauses,axis=1)\n",
    "      if priming_ABB: prime_ABB = np.insert(combs_AB[prime_order][:,(0,1,1)],3,pauses,axis=1)\n",
    "\n",
    "  used_tokens = np.unique(np.append(np.append(prime_AAB, prime_ABA),prime_ABB))\n",
    "\n",
    "  probe_AAB = probe_ABA = probe_ABB = probe_ABC = np.array([]).astype('int64')\n",
    "\n",
    "  if probes_eql_primes:\n",
    "    if priming_AAB: \n",
    "      probe_AAB = prime_AAB[:,:3]\n",
    "    if priming_ABA: \n",
    "      probe_ABA = prime_ABA[:,:3]\n",
    "    if priming_ABB: \n",
    "      probe_ABB = prime_ABB[:,:3]\n",
    "    probing_ABCa = False\n",
    "    probing_ABCb = False\n",
    "\n",
    "  elif custom_probes:\n",
    "    if real_probes:\n",
    "      probing_ABCa = False\n",
    "      probing_ABCb = False\n",
    "\n",
    "      if probing_AAB: probe_AAB = pmi_index[pmi_index['Pattern'].str.contains('AAB')][:num_probe*2].reset_index(drop=True).filter(\n",
    "          items=['Position 1', 'Position 2','Position 3']).to_numpy()[(probe_positions),:]\n",
    "      if probing_ABA: probe_ABA = pmi_index[pmi_index['Pattern'].str.contains('ABA')][:num_probe*2].reset_index(drop=True).filter(\n",
    "          items=['Position 1', 'Position 2','Position 3']).to_numpy()[(probe_positions),:]\n",
    "      if probing_ABB: probe_ABB = pmi_index[pmi_index['Pattern'].str.contains('ABB')][:num_probe*2].reset_index(drop=True).filter(\n",
    "          items=['Position 1', 'Position 2','Position 3']).to_numpy()[(probe_positions),:]\n",
    "\n",
    "    else: print('Fake probe condition not implemented yet')\n",
    "\n",
    "  \n",
    "  else: # random probes\n",
    "    ids_probe_a = token_selector(num_probe_tokens,np.concatenate((special_tokens, used_tokens), axis=0))\n",
    "    ids_probe_b = token_selector(num_probe_tokens,np.concatenate((special_tokens, used_tokens), axis=0))\n",
    "    \n",
    "    combs_AB = np.transpose((np.repeat(ids_probe_a, len(ids_probe_b)), np.tile(ids_probe_b, len(ids_probe_a))))\n",
    "\n",
    "    if probing_AAB: probe_AAB = combs_AB[:,(0,0,1)]\n",
    "    if probing_ABA: probe_ABA = combs_AB[:,(0,1,0)]\n",
    "    if probing_ABB: probe_ABB = combs_AB[:,(0,1,1)]\n",
    "    if probing_ABCa: probe_ABC = np.roll(np.insert(combs_AB,0,np.roll(combs_AB[:,0],-num_probe_tokens),axis=1),-1,axis=1)\n",
    "    if probing_ABCb: probe_ABC = np.roll(np.insert(combs_AB,0,np.roll(combs_AB[:,1],-1),axis=1),-1,axis=1)\n",
    "\n",
    "  metainfo += str(f'\\n---------\\nCycle: {i_exp}\\n---------\\n')\n",
    "  metainfo += str(f'Prime AAB:\\n{prime_AAB}\\n\\nProbe AAB:\\n{probe_AAB}\\n\\n__________\\n')\n",
    "  metainfo += str(f'Prime ABA:\\n{prime_ABA}\\n\\nProbe ABA:\\n{probe_ABA}\\n\\n__________\\n')\n",
    "  metainfo += str(f'Prime ABB:\\n{prime_ABB}\\n\\nProbe ABB:\\n{probe_ABB}\\n\\n__________\\n')\n",
    "  metainfo += str(f'Probe ABC:\\n{probe_ABC}')\n",
    "\n",
    "  mask_v = np.repeat(special_tokens[1],num_probe)\n",
    "\n",
    "  # Normalization probs (without priming)\n",
    "  if probing_AAB: \n",
    "    probabilities[0,0] = np.diag(priming_probs(np.insert( probe_AAB ,1,mask_v,axis=1)[:,:2], probe_AAB[:,1] )) # Pos 2 (Aa) norm\n",
    "    probabilities[0,2] = np.diag(priming_probs(np.insert( probe_AAB ,2,mask_v,axis=1)[:,:3], probe_AAB[:,2] )) # Pos 3 (AAb) norm\n",
    "    \n",
    "  if probing_ABA or probing_ABB or probing_ABCa or probing_ABCb:\n",
    "    probabilities[0,1] = np.diag(priming_probs(np.insert( probe_ABA ,1,mask_v,axis=1)[:,:2], probe_ABA[:,1] )) # Pos 2 (Ab) norm\n",
    "    if probing_ABA: probabilities[0,3] = np.diag(priming_probs(np.insert( probe_ABA ,2,mask_v,axis=1)[:,:3], probe_ABA[:,2] )) # Pos 3 (ABa) norm\n",
    "    if probing_ABB: probabilities[0,4] = np.diag(priming_probs(np.insert( probe_ABB ,2,mask_v,axis=1)[:,:3], probe_ABB[:,2] )) # Pos 3 (ABb) norm\n",
    "    if probing_ABCa or probing_ABCb: \n",
    "      probabilities[0,5] = np.diag(priming_probs(np.insert( probe_ABC ,2,mask_v,axis=1)[:,:3], probe_ABC[:,2] )) # Pos 3 (ABc) norm\n",
    "  \n",
    "  \n",
    "  # Primed probs\n",
    "  all_primes = np.array([prime_AAB, prime_ABA, prime_ABB])\n",
    "  for prime_i in range(1,len(all_primes)+1):\n",
    "    \n",
    "    if len(all_primes[prime_i-1]) != 0:\n",
    "      prime_temp = np.tile(all_primes[prime_i-1].flatten(),(num_probe,1))\n",
    "\n",
    "      if probing_AAB: \n",
    "        probabilities[prime_i,0] = np.diag(priming_probs(np.insert(np.roll(np.insert(prime_temp,0,mask_v,axis=1),-1,axis=1),-1, probe_AAB[:,0] ,axis=1), probe_AAB[:,1] )) # Pos 2 (Aa) prime_temp\n",
    "        probabilities[prime_i,2] = np.diag(priming_probs(np.insert(np.insert(\n",
    "            np.roll(np.insert(prime_temp,0,mask_v,axis=1),-1,axis=1),-1, probe_AAB[:,0] ,axis=1),-1, probe_AAB[:,1] ,axis=1), probe_AAB[:,2] )) # Pos 3 (AAb) prime_temp\n",
    "\n",
    "      if probing_ABA or probing_ABB or probing_ABCa or probing_ABCb:\n",
    "        probabilities[prime_i,1] = np.diag(priming_probs(np.insert(np.roll(np.insert(prime_temp,0,mask_v,axis=1),-1,axis=1),-1, probe_ABA[:,0] ,axis=1), probe_ABA[:,1] )) # Pos 2 (Ab) prime_temp\n",
    "        if probing_ABA: probabilities[prime_i,3] = np.diag(priming_probs(np.insert(np.insert(\n",
    "            np.roll(np.insert(prime_temp,0,mask_v,axis=1),-1,axis=1),-1, probe_ABA[:,0] ,axis=1),-1, probe_ABA[:,1] ,axis=1), probe_ABA[:,2] )) # Pos 3 (ABa) prime_temp\n",
    "        if probing_ABB: probabilities[prime_i,4] = np.diag(priming_probs(np.insert(np.insert(\n",
    "            np.roll(np.insert(prime_temp,0,mask_v,axis=1),-1,axis=1),-1, probe_ABB[:,0] ,axis=1),-1, probe_ABB[:,1] ,axis=1), probe_ABB[:,2] )) # Pos 3 (ABb) prime_temp\n",
    "        if probing_ABCa or probing_ABCb:\n",
    "          probabilities[prime_i,5] = np.diag(priming_probs(np.insert(np.insert(\n",
    "            np.roll(np.insert(prime_temp,0,mask_v,axis=1),-1,axis=1),-1, probe_ABC[:,0] ,axis=1),-1, probe_ABC[:,1] ,axis=1), probe_ABC[:,2] )) # Pos 3 (ABc) prime_temp\n",
    "\n",
    "    df_temp = pd.DataFrame(data={'P(C|prime,A,B)': probabilities[prime_i,5]}) # primed values...\n",
    "    df_temp.insert(loc=0, column='P(B|prime,A,B)', value=probabilities[prime_i,4])\n",
    "    df_temp.insert(loc=0, column='P(A|prime,A,B)', value=probabilities[prime_i,3])\n",
    "    df_temp.insert(loc=0, column='P(B|prime,A,A)', value=probabilities[prime_i,2])\n",
    "    df_temp.insert(loc=0, column='P(B|prime,A)', value=probabilities[prime_i,1])\n",
    "    df_temp.insert(loc=0, column='P(A|prime,A)', value=probabilities[prime_i,0])\n",
    "    df_temp.insert(loc=0, column='P(C|A,B)', value=probabilities[0,5]) # norm values...\n",
    "    df_temp.insert(loc=0, column='P(B|A,B)', value=probabilities[0,4])\n",
    "    df_temp.insert(loc=0, column='P(A|A,B)', value=probabilities[0,3])\n",
    "    df_temp.insert(loc=0, column='P(B|A,A)', value=probabilities[0,2])\n",
    "    df_temp.insert(loc=0, column='P(B|A)', value=probabilities[0,1])\n",
    "    df_temp.insert(loc=0, column='P(A|A)', value=probabilities[0,0])\n",
    "    if probing_ABCa or probing_ABCb: df_temp.insert(loc=0, column='C', value=probe_ABC[:,2]) # Token information...\n",
    "    else: df_temp.insert(loc=0, column='C', value=np.full((num_probe,),None))\n",
    "    df_temp.insert(loc=0, column='B', value=probe_ABA[:,1]) # CONDITIONS required if not all probes are selected!!!\n",
    "    df_temp.insert(loc=0, column='A', value=probe_ABA[:,0]) # CONDITIONS required if not all probes are selected!!!\n",
    "    df_temp.insert(loc=0, column='P(ABC|prime)', value= # Results...\n",
    "                   np.divide(probabilities[prime_i,1],probabilities[0,1])*np.divide(probabilities[prime_i,5],probabilities[0,5]))\n",
    "    df_temp.insert(loc=0, column='P(ABB|prime)', value=\n",
    "                   np.divide(probabilities[prime_i,1],probabilities[0,1])*np.divide(probabilities[prime_i,4],probabilities[0,4]))\n",
    "    df_temp.insert(loc=0, column='P(ABA|prime)', value=\n",
    "                   np.divide(probabilities[prime_i,1],probabilities[0,1])*np.divide(probabilities[prime_i,3],probabilities[0,3]))\n",
    "    df_temp.insert(loc=0, column='P(AAB|prime)', value=\n",
    "                   np.divide(probabilities[prime_i,0],probabilities[0,0])*np.divide(probabilities[prime_i,2],probabilities[0,2]))\n",
    "\n",
    "    if prime_i == 1: df_AAB_priming = df_AAB_priming.append(df_temp,ignore_index = True)\n",
    "    if prime_i == 2: df_ABA_priming = df_ABA_priming.append(df_temp,ignore_index = True)\n",
    "    if prime_i == 3: df_ABB_priming = df_ABB_priming.append(df_temp,ignore_index = True)\n",
    "    if prime_i > 3: print('Prime number inconsistency!')\n",
    "\n",
    "settings = f'--------\\nSETTINGS\\n--------\\nNLP Model: {model_name}\\n'\n",
    "settings += f'Priming conditions: \\n\\tAAB: {priming_AAB} \\n\\tABA: {priming_ABA} \\n\\tABB: {priming_ABB}\\n'\n",
    "settings += f'Probing conditions: \\n\\tAAB: {probing_AAB} \\n\\tABA: {probing_ABA} \\n\\tABB: {probing_ABB} \\n\\tABA\\'(ABCa): {probing_ABCa} \\n\\tABB\\'(ABCb): {probing_ABCb}\\n'\n",
    "settings += f'Custom primes (PPMI): {custom_primes}\\nCustom probes (PPMI): {custom_probes}\\n'\n",
    "settings += f'Primes = probes: {probes_eql_primes}\\n'\n",
    "settings += f'Experiment cycles: {experiment_cycles}\\n'\n",
    "\n",
    "metainfo = settings + metainfo\n",
    "time_st = str(datetime.now())\n",
    "\n",
    "# Save metainfo\n",
    "f = open(time_st+'_metainfo.txt','w')\n",
    "f.write(metainfo)\n",
    "f.close()\n",
    "\n",
    "df_AAB_priming.to_csv(time_st+'_AAB_prime.csv')\n",
    "df_ABA_priming.to_csv(time_st+'_ABA_prime.csv')\n",
    "df_ABB_priming.to_csv(time_st+'_ABB_prime.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQD8cN5tYR82"
   },
   "source": [
    "# Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "wXyBXQF6v-Ou",
    "outputId": "37d00ec5-1949-4242-ab8c-1066ff85e857"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_1122e658-cf46-474b-85e6-ca11c58aae34\", \"2021-08-13 15:00:23.636565_metainfo.txt\", 629957)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_8fbe0de6-90a7-4cfe-96ac-03a31231181a\", \"2021-08-13 15:00:23.636565_AAB_prime.csv\", 765313)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_4681c58e-0b55-474a-9f29-160082d28a13\", \"2021-08-13 15:00:23.636565_ABA_prime.csv\", 756609)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_8c99f06a-5c8c-4d01-abaf-4f7dc7714428\", \"2021-08-13 15:00:23.636565_ABB_prime.csv\", 759169)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "files.download(time_st+'_metainfo.txt')\n",
    "files.download(time_st+'_AAB_prime.csv')\n",
    "files.download(time_st+'_ABA_prime.csv')\n",
    "files.download(time_st+'_ABB_prime.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Experiment_1_for_all_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
